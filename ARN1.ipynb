{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fad027-f08e-49fb-ae2f-8590a608458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm #progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6d7dc-e2ea-4dff-b88a-83e1f4623c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class artificial_neuron:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def initialisation(self, n0 ,n1 ,n2):\n",
    "        w1 = np.random.randn(n1 , n0)\n",
    "        b1 = np.random.randn(n1, 1)\n",
    "        w1 = np.random.randn(n2 , n1)\n",
    "        b1 = np.random.randn(n2, 1)\n",
    "\n",
    "        parameter = {\n",
    "            'w1' : w1,\n",
    "            'b1' : b1,\n",
    "            'w2' : w2,\n",
    "            'b2' : b2  \n",
    "        }\n",
    "        return parameter\n",
    "        \n",
    "    def model(self, x, parameter):\n",
    "        \n",
    "        w1 = parameter['w1']\n",
    "        b1 = parameter['b1']\n",
    "        w2 = parameter['w2']\n",
    "        b2 = parameter['b2']\n",
    "        \n",
    "        z1 = w1.dot(x) + b1\n",
    "        A1 = 1 / (1 + np.exp(-z1))\n",
    "        z2 = w2.dot(x) + b2\n",
    "        A2 = 1 / (1 + np.exp(-z2))\n",
    "\n",
    "        activation = {\n",
    "            'A1' : A1,\n",
    "            'A2' : A2, \n",
    "        }\n",
    "        \n",
    "        return activation \n",
    "\n",
    "    def log_loss(self, A, y):\n",
    "        epsilon = 1e-15\n",
    "        return 1/len(y) * np.sum(-y * np.log(A + epsilon) - (1 - y) * np.log(1 - A + epsilon))\n",
    "\n",
    "    def back_propagation(self, x, y, activation , parameter):\n",
    "        A1 = activation['A1']\n",
    "        A2 = activation['A2']\n",
    "        w2 = parameter['w2']\n",
    "\n",
    "        m = y.shape[1]\n",
    "        dz2 = A2 - y\n",
    "        dw2 = 1/m * dz2.dot(A1.T)\n",
    "        db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "        dz1 = A2 - y\n",
    "        dw1 = 1/m * dz1.dot(x.T)\n",
    "        db1 = 1/m * np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "        gradients = {\n",
    "            'dw1' : dw1,\n",
    "            'db1' : b1, \n",
    "            'dw2' : dw2,\n",
    "            'db2' : b2,\n",
    "        }\n",
    "        return gradients    \n",
    "\n",
    "    def update(self, parameter, gradients, learning_rate):\n",
    "        w2 = parameter['w2']\n",
    "        b2 = parameter['b2']\n",
    "        w1 = parameter['w1']\n",
    "        b1 = parameter['b1']\n",
    "\n",
    "        dw1 = gradients['dw1']\n",
    "        db1 = gradients['db1']\n",
    "        dw2 = gradients['dw2']\n",
    "        db2 = gradients['db2']\n",
    "        \n",
    "        w1 = w1 - learning_rate * dw1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        w2 = w2 - learning_rate * dw2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "\n",
    "        parameter = {\n",
    "            'w1' : w1,\n",
    "            'b1' : b1,\n",
    "            'w2' : w2,\n",
    "            'b2' : b2  \n",
    "        }\n",
    "        return parameter\n",
    "\n",
    "    def predict(self, x, parameter):\n",
    "        activation = self.model(x, parameter)\n",
    "        A2 = activation['A2']\n",
    "        return A2 >= 0.5\n",
    "        \n",
    "    def ARN(self, learning_rate=0.1, n_iter=100):  # Remove x,y parameters\n",
    "        # Use self.x and self.y that were set in __init__\n",
    "        w, b = self.initialisation(self.x)\n",
    "        loss = []\n",
    "        acc = []\n",
    "        #loop learning\n",
    "        for i in tqdm(range(n_iter)): # show the bare of progretion\n",
    "            A = self.model(self.x, w, b)\n",
    "            if i %10 == 0:\n",
    "                #calculate of loss\n",
    "                loss.append(self.log_loss(A, self.y))\n",
    "    \n",
    "                #calculate of accuracy\n",
    "                y_pred = self.predict(self.x, w, b)\n",
    "                acc.append(accuracy_score(self.y, y_pred))\n",
    "            \n",
    "            #update\n",
    "            dw, db = self.gradients(A, self.x, self.y)\n",
    "            w, b = self.update(dw, db, w, b, learning_rate)\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(loss)\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(acc)\n",
    "        plt.show()\n",
    "\n",
    "        return (w, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
